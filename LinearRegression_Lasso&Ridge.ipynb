{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Name: Venkata Avinash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Logistic regression is used to find the probability of event=Success and event=Failure. We should use logistic regression when the dependent variable is binary (0/ 1, True/ False, Yes/ No) in nature. Here the value of Y ranges from 0 to 1 and it can represented by following equation.*\n",
    "\n",
    "**odds= p/ (1-p) = probability of event occurrence / probability of no event occurrence**\n",
    "\n",
    "**ln(odds) = ln(p/(1-p))**\n",
    "\n",
    "**logit(p) = ln(p/(1-p)) = b0+b1X1+b2X2+b3X3....+bkXk**\n",
    "\n",
    "*Above, p is the probability of presence of the characteristic of interest. A question that you should ask here is “why have we used log in the equation?”.*\n",
    "\n",
    "*Since we are working here with a binomial distribution (dependent variable), we need to choose a link function which is best suited for this distribution. And, it is logit function. In the equation above, the parameters are chosen to maximize the likelihood of observing the sample values rather than minimizing the sum of squared errors (like in ordinary regression).*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<image src=\"log.png\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<image src=\"log.png\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Logistic regression is named for the function used at the core of the method, the logistic function. In linear regression, the outcome (dependent variable) is continuous. It can have any one of an infinite number of possible values. In logistic regression, the outcome (dependent variable) has only a limited number of possible values. Logistic Regression is used when response variable is categorical in nature.*\n",
    "\n",
    "*The logistic function, also called the sigmoid function is an S-shaped curve that can take any real-valued number and map it into a value between 0 and 1, but never exactly at those limits.*\n",
    "\n",
    "\n",
    "*Logistic regression uses an equation as the representation, similar to linear regression. The central premise of Logistic Regression is the assumption that your input space can be separated into two nice ‘regions’, one for each class, by a linear(read: straight) boundary. Your data must be linearly seperable in n dimensions*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<image src=\"log2.png\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<image src=\"log2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important Points:**\n",
    "- It is widely used for **classification** problems\n",
    "- Logistic regression doesn’t require linear relationship between dependent and independent variables.  It can handle various types of relationships because it applies a non-linear log transformation to the predicted odds ratio\n",
    "- To avoid over fitting and under fitting, we should include all significant variables. A good approach to ensure this practice is to use a step wise method to estimate the logistic regression\n",
    "- It requires **large sample sizes** because maximum likelihood estimates are less powerful at low sample sizes than ordinary least square\n",
    "- The independent variables should not be correlated with each other i.e. **no multi collinearity.** However, we have the options to include interaction effects of categorical variables in the analysis and in the model.\n",
    "- If the values of dependent variable is ordinal, then it is called as **Ordinal logistic regression**\n",
    "- If dependent variable is multi class then it is known as **Multinomial Logistic regression.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Importing libraries.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 12, 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset : Wine recognition data**\n",
    "\n",
    "*These data are the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars.The analysis determined the quantities of 13 constituents found in each of the three types of wines.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0      1     2     3     4    5     6     7     8     9     10    11    12  \\\n",
       "0   1  14.23  1.71  2.43  15.6  127  2.80  3.06  0.28  2.29  5.64  1.04  3.92   \n",
       "1   1  13.20  1.78  2.14  11.2  100  2.65  2.76  0.26  1.28  4.38  1.05  3.40   \n",
       "2   1  13.16  2.36  2.67  18.6  101  2.80  3.24  0.30  2.81  5.68  1.03  3.17   \n",
       "3   1  14.37  1.95  2.50  16.8  113  3.85  3.49  0.24  2.18  7.80  0.86  3.45   \n",
       "4   1  13.24  2.59  2.87  21.0  118  2.80  2.69  0.39  1.82  4.32  1.04  2.93   \n",
       "\n",
       "     13  \n",
       "0  1065  \n",
       "1  1050  \n",
       "2  1185  \n",
       "3  1480  \n",
       "4   735  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ### Load and prepare the wine dataset\n",
    "\n",
    "# read in the dataset\n",
    "url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data'\n",
    "wine = pd.read_csv(url, header=None)\n",
    "wine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    71\n",
       "1    59\n",
       "3    48\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the response variable\n",
    "wine[0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define X and y\n",
    "X = wine.drop(0, axis=1)\n",
    "y = wine[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split into training and testing sets\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1000000000.0, class_weight=None, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ### Logistic regression (unregularized)\n",
    "\n",
    "# build a logistic regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(C=1e9)\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -3.78736769e+00   4.67404435e+00   1.35566674e+01  -2.56780208e+00\n",
      "    9.94234266e-02  -4.66366672e+00   9.32254676e+00   2.64475215e+00\n",
      "   -1.02529022e+01  -2.15290823e+00  -1.62698597e+00   6.48024952e+00\n",
      "    6.18085493e-02]\n",
      " [  6.51497931e+00  -6.32567232e+00  -2.10346336e+01   2.04206275e+00\n",
      "    8.21697639e-03   3.00794699e+00   6.04048700e+00   3.27672865e+00\n",
      "    7.09827242e+00  -9.51649745e+00   4.02292480e+00  -8.89168128e+00\n",
      "   -5.84955207e-02]\n",
      " [ -9.70213425e-01   2.08271772e+00   9.41518259e-01   2.38015929e-01\n",
      "   -2.49625850e-03  -9.80976091e-01  -6.54884298e+00  -4.83304557e-01\n",
      "   -2.65889488e+00   2.57460650e+00  -1.30416276e+00  -2.34299280e+00\n",
      "    9.48532261e-03]]\n"
     ]
    }
   ],
   "source": [
    "# examine the coefficients\n",
    "print(logreg.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  3.74367140e-09   2.77280287e-12   9.99999996e-01]\n",
      " [  1.92307711e-15   1.00000000e+00   4.32233774e-10]\n",
      " [  9.99999906e-01   1.64538949e-13   9.37827691e-08]\n",
      " [  4.56199707e-08   9.99999830e-01   1.24112647e-07]\n",
      " [  9.99730799e-01   1.36949166e-25   2.69201406e-04]\n",
      " [  4.73996999e-13   3.59037244e-08   9.99999964e-01]\n",
      " [  9.99842418e-01   1.12959212e-06   1.56452095e-04]\n",
      " [  9.99999720e-01   1.16008720e-28   2.80169301e-07]\n",
      " [  1.36326031e-15   4.67735396e-16   1.00000000e+00]\n",
      " [  7.02023413e-15   9.99997008e-01   2.99209787e-06]\n",
      " [  9.99967659e-01   9.98581869e-15   3.23408886e-05]\n",
      " [  9.92807472e-01   7.19230165e-03   2.26351122e-07]\n",
      " [  3.65399753e-18   1.00000000e+00   1.12558699e-10]\n",
      " [  9.99999998e-01   2.08746436e-16   1.72639210e-09]\n",
      " [  2.72089648e-08   9.99999972e-01   3.47691766e-10]\n",
      " [  8.79275722e-14   9.99999959e-01   4.13218799e-08]\n",
      " [  2.51842280e-26   4.58258536e-01   5.41741464e-01]\n",
      " [  9.99994386e-01   5.58697696e-06   2.65299468e-08]\n",
      " [  9.71667813e-13   9.99999930e-01   7.00543322e-08]\n",
      " [  9.99778297e-01   2.93006759e-25   2.21703375e-04]\n",
      " [  9.99999731e-01   2.30761969e-18   2.69288101e-07]\n",
      " [  2.24297782e-14   1.00000000e+00   2.86964433e-11]\n",
      " [  1.22445176e-08   9.69528256e-01   3.04717317e-02]\n",
      " [  1.17878872e-04   9.99882118e-01   3.47383080e-09]\n",
      " [  9.99995846e-01   4.15279402e-06   1.08758382e-09]\n",
      " [  1.18582113e-11   5.08648852e-10   9.99999999e-01]\n",
      " [  9.99999989e-01   7.42886720e-09   3.11850661e-09]\n",
      " [  9.99999850e-01   2.08346381e-09   1.47480171e-07]\n",
      " [  9.99986848e-01   1.18652966e-16   1.31520721e-05]\n",
      " [  2.15601399e-10   4.16501500e-23   1.00000000e+00]\n",
      " [  3.89453492e-10   1.00000000e+00   4.08139495e-11]\n",
      " [  6.21883624e-28   1.72651713e-23   1.00000000e+00]\n",
      " [  2.98185651e-20   6.39759064e-23   1.00000000e+00]\n",
      " [  9.99995751e-01   1.98092396e-24   4.24859303e-06]\n",
      " [  3.15537633e-01   6.84462366e-01   1.05425469e-09]\n",
      " [  7.74763373e-10   9.99999995e-01   3.89768702e-09]\n",
      " [  1.13400915e-15   9.99932287e-01   6.77129897e-05]\n",
      " [  4.18761567e-03   9.95812370e-01   1.47308241e-08]\n",
      " [  3.38823341e-11   9.99999997e-01   3.22315108e-09]\n",
      " [  9.99998280e-01   7.12085145e-10   1.71932160e-06]\n",
      " [  9.99999019e-01   6.01549456e-14   9.80676886e-07]\n",
      " [  1.74579052e-08   9.99903424e-01   9.65586229e-05]\n",
      " [  4.37431317e-18   2.27111213e-09   9.99999998e-01]\n",
      " [  9.99999985e-01   1.88087219e-12   1.45054923e-08]\n",
      " [  9.99999920e-01   3.95305899e-11   7.96640082e-08]]\n"
     ]
    }
   ],
   "source": [
    "# generate predicted probabilities\n",
    "y_pred_prob = logreg.predict_proba(X_test)\n",
    "print(y_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log loss (no penalty) = 0.404198941498\n"
     ]
    }
   ],
   "source": [
    "# calculate log loss\n",
    "print('Log loss (no penalty) =',metrics.log_loss(y_test, y_pred_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (no penalty) = 0.955555555556\n"
     ]
    }
   ],
   "source": [
    "y_pred = logreg.predict(X_test)\n",
    "# Access accuracy\n",
    "print('Accuracy (no penalty) =', metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ### Logistic regression (regularized)\n",
    "# - **C:** must be positive, decrease for more regularization\n",
    "# - **penalty:** l1 (lasso) or l2 (ridge)\n",
    "\n",
    "# standardize X_train and X_test\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression With L1 Penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.21031904  0.          0.          0.          0.          0.\n",
      "   0.48878885  0.          0.          0.          0.          0.15167683\n",
      "   1.47744073]\n",
      " [-0.65696892 -0.05643186 -0.11388972  0.          0.          0.          0.\n",
      "   0.          0.         -0.7386228   0.24358768  0.         -0.6340503 ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "  -0.84213403  0.          0.          0.61549502 -0.49012227 -0.30451803\n",
      "   0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# logistic regression with L1 penalty (C must be positive, smaller means more regularization)\n",
    "# try C=0.1 with L1 penalty\n",
    "logreg = LogisticRegression(C=0.1, penalty='l1')\n",
    "logreg.fit(X_train_scaled, y_train)\n",
    "print(logreg.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Loss (L1 penalty) = 0.362243909137\n"
     ]
    }
   ],
   "source": [
    "# generate predicted probabilities and calculate log loss\n",
    "y_pred_prob = logreg.predict_proba(X_test_scaled)\n",
    "print('Log Loss (L1 penalty) =',metrics.log_loss(y_test, y_pred_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (L1 penalty) = 0.977777777778\n"
     ]
    }
   ],
   "source": [
    "y_pred_l1 = logreg.predict(X_test_scaled)\n",
    "# Access accuracy\n",
    "print('Accuracy (L1 penalty) =', metrics.accuracy_score(y_test, y_pred_l1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression With L2 Penalty "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.59163934  0.06886667  0.33592964 -0.49616684  0.111539    0.21570086\n",
      "   0.40524509 -0.15526139 -0.02534651  0.05399014  0.14877346  0.42327938\n",
      "   0.89815007]\n",
      " [-0.73545676 -0.32942948 -0.47995296  0.294866   -0.1500246   0.04264373\n",
      "   0.14500586  0.07250763  0.17409795 -0.70726652  0.4128986   0.09997212\n",
      "  -0.81284365]\n",
      " [ 0.20136567  0.30989025  0.15977925  0.18867218  0.04204443 -0.27108109\n",
      "  -0.55886639  0.07486943 -0.17471153  0.68266464 -0.52385748 -0.49566967\n",
      "  -0.02565631]]\n"
     ]
    }
   ],
   "source": [
    "# logistic regression with L2 penalty (C must be positive, smaller means more regularization)\n",
    "# try C=0.1 with L2 penalty\n",
    "logreg = LogisticRegression(C=0.1, penalty='l2')\n",
    "logreg.fit(X_train_scaled, y_train)\n",
    "print(logreg.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Loss (L1 penalty) = 0.244588324539\n"
     ]
    }
   ],
   "source": [
    "# generate predicted probabilities and calculate log loss\n",
    "y_pred_prob = logreg.predict_proba(X_test_scaled)\n",
    "print('Log Loss (L1 penalty) =',metrics.log_loss(y_test, y_pred_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (L2 penalty) = 1.0\n"
     ]
    }
   ],
   "source": [
    "y_pred_l2 = logreg.predict(X_test_scaled)\n",
    "# Access accuracy\n",
    "print('Accuracy (L2 penalty) =', metrics.accuracy_score(y_test, y_pred_l2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pipeline of StandardScaler and LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "pipe = make_pipeline(StandardScaler(), LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise',\n",
       "       estimator=Pipeline(steps=[('standardscaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('logisticregression', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))]),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'logisticregression__C': array([  1.00000e-02,   1.00000e-01,   1.00000e+00,   1.00000e+01,\n",
       "         1.00000e+02]), 'logisticregression__penalty': ['l1', 'l2']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring='neg_log_loss',\n",
       "       verbose=0)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grid search for best combination of C and penalty\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "C_range = 10.**np.arange(-2, 3)\n",
    "penalty_options = ['l1', 'l2']\n",
    "param_grid = dict(logisticregression__C=C_range, logisticregression__penalty=penalty_options)\n",
    "grid = GridSearchCV(pipe, param_grid, cv=10, scoring='neg_log_loss')\n",
    "grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[mean: -1.09861, std: 0.00000, params: {'logisticregression__C': 0.01, 'logisticregression__penalty': 'l1'},\n",
       " mean: -0.62547, std: 0.03037, params: {'logisticregression__C': 0.01, 'logisticregression__penalty': 'l2'},\n",
       " mean: -0.35490, std: 0.06892, params: {'logisticregression__C': 0.10000000000000001, 'logisticregression__penalty': 'l1'},\n",
       " mean: -0.26801, std: 0.04840, params: {'logisticregression__C': 0.10000000000000001, 'logisticregression__penalty': 'l2'},\n",
       " mean: -0.09437, std: 0.06114, params: {'logisticregression__C': 1.0, 'logisticregression__penalty': 'l1'},\n",
       " mean: -0.10371, std: 0.04894, params: {'logisticregression__C': 1.0, 'logisticregression__penalty': 'l2'},\n",
       " mean: -0.05891, std: 0.06492, params: {'logisticregression__C': 10.0, 'logisticregression__penalty': 'l1'},\n",
       " mean: -0.06174, std: 0.05651, params: {'logisticregression__C': 10.0, 'logisticregression__penalty': 'l2'},\n",
       " mean: -0.07036, std: 0.09876, params: {'logisticregression__C': 100.0, 'logisticregression__penalty': 'l1'},\n",
       " mean: -0.06443, std: 0.08409, params: {'logisticregression__C': 100.0, 'logisticregression__penalty': 'l2'}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print all log loss scores\n",
    "grid.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.05890603959490997\n",
      "{'logisticregression__C': 10.0, 'logisticregression__penalty': 'l1'}\n"
     ]
    }
   ],
   "source": [
    "# examine the best model\n",
    "print(grid.best_score_)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regularization is a very important technique in machine learning to prevent overfitting. Mathematically speaking, it adds a *regularization* term in order to prevent the coefficients to fit so perfectly to overfit. The difference between the L1 and L2 is just that L2 is the sum of the square of the weights, while L1 is just the sum of the weights.As follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L1 regularization on least squares:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<image src=\"L1.png\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<image src=\"L1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L2 regularization on least squares:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<image src=\"L2.png\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<image src=\"L2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The difference between their properties can be promptly summarized as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<image src=\"diff.png\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<image src=\"diff.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=100.00\n",
      "Sparsity with L1 penalty: 4.69%\n",
      "score with L1 penalty: 0.9098\n",
      "Sparsity with L2 penalty: 4.69%\n",
      "score with L2 penalty: 0.9098\n",
      "C=1.00\n",
      "Sparsity with L1 penalty: 9.38%\n",
      "score with L1 penalty: 0.9093\n",
      "Sparsity with L2 penalty: 4.69%\n",
      "score with L2 penalty: 0.9093\n",
      "C=0.01\n",
      "Sparsity with L1 penalty: 85.94%\n",
      "score with L1 penalty: 0.8625\n",
      "Sparsity with L2 penalty: 4.69%\n",
      "score with L2 penalty: 0.8915\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiIAAAJBCAYAAAB71N7eAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x5392cb73c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "digits = datasets.load_digits()\n",
    "\n",
    "X, y = digits.data, digits.target\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "# classify small against large digits\n",
    "y = (y > 4).astype(np.int)\n",
    "\n",
    "\n",
    "# Set regularization parameter\n",
    "for i, C in enumerate((100, 1, 0.01)):\n",
    "    # turn down tolerance for short training time\n",
    "    clf_l1_LR = LogisticRegression(C=C, penalty='l1', tol=0.01)\n",
    "    clf_l2_LR = LogisticRegression(C=C, penalty='l2', tol=0.01)\n",
    "    clf_l1_LR.fit(X, y)\n",
    "    clf_l2_LR.fit(X, y)\n",
    "\n",
    "    coef_l1_LR = clf_l1_LR.coef_.ravel()\n",
    "    coef_l2_LR = clf_l2_LR.coef_.ravel()\n",
    "\n",
    "    # coef_l1_LR contains zeros due to the\n",
    "    # L1 sparsity inducing norm\n",
    "\n",
    "    sparsity_l1_LR = np.mean(coef_l1_LR == 0) * 100\n",
    "    sparsity_l2_LR = np.mean(coef_l2_LR == 0) * 100\n",
    "\n",
    "    print(\"C=%.2f\" % C)\n",
    "    print(\"Sparsity with L1 penalty: %.2f%%\" % sparsity_l1_LR)\n",
    "    print(\"score with L1 penalty: %.4f\" % clf_l1_LR.score(X, y))\n",
    "    print(\"Sparsity with L2 penalty: %.2f%%\" % sparsity_l2_LR)\n",
    "    print(\"score with L2 penalty: %.4f\" % clf_l2_LR.score(X, y))\n",
    "\n",
    "    l1_plot = plt.subplot(3, 2, 2 * i + 1)\n",
    "    l2_plot = plt.subplot(3, 2, 2 * (i + 1))\n",
    "    if i == 0:\n",
    "        l1_plot.set_title(\"L1 penalty\")\n",
    "        l2_plot.set_title(\"L2 penalty\")\n",
    "\n",
    "    l1_plot.imshow(np.abs(coef_l1_LR.reshape(8, 8)), interpolation='nearest',\n",
    "                   cmap='binary', vmax=1, vmin=0)\n",
    "    l2_plot.imshow(np.abs(coef_l2_LR.reshape(8, 8)), interpolation='nearest',\n",
    "                   cmap='binary', vmax=1, vmin=0)\n",
    "    plt.text(-8, 3, \"C = %.2f\" % C)\n",
    "\n",
    "    l1_plot.set_xticks(())\n",
    "    l1_plot.set_yticks(())\n",
    "    l2_plot.set_xticks(())\n",
    "    l2_plot.set_yticks(())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " **Advantages of regularized linear models:**\n",
    " \n",
    " - Better performance\n",
    " - L1 regularization performs automatic feature selection\n",
    " - Useful for high-dimensional problems (p > n)\n",
    "\n",
    "**Disadvantages of regularized linear models:**\n",
    " \n",
    " - Tuning is required\n",
    " - Feature scaling is recommended\n",
    " - Less interpretable (due to feature scaling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References**:\n",
    "\n",
    "- http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html\n",
    "- http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html\n",
    "- http://scikit-learn.org/stable/auto_examples/linear_model/plot_logistic_l1_l2_sparsity.html#sphx-glr-auto-examples-linear-model-plot-logistic-l1-l2-sparsity-py\n",
    "- https://www.analyticsvidhya.com/blog/2015/08/comprehensive-guide-regression/\n",
    "- https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-ridge-lasso-regression-python/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
